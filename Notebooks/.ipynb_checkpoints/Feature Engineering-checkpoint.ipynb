{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import json\n",
    "import string\n",
    "import itertools\n",
    "import numpy as np\n",
    "from text2num import text2num\n",
    "from textstat.textstat import textstat\n",
    "from nltk import word_tokenize, pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "The following notebook will focus on generating as much hand-crafted features as possible through various methods and techniques. This is simply a workbech we will be using to define each of the function. In the end we hope to parallelize this process to increase the processing speed of this preprocessing phase.\n",
    "\n",
    "We will follow through this also by performing an analysis on a corresponding feature importance analysis to evaluate whether to utilize them or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Application Directory Constants\n",
    "DATA_DIR = '../Data/dataset/'\n",
    "\n",
    "# Load Dataset\n",
    "instance_raw = open(DATA_DIR+'instances_train.jsonl', 'rb').read().replace('\\\"', '\"').split('\\n')[:-1]\n",
    "train_X = map(json.loads, instance_raw)\n",
    "\n",
    "truth_raw = open(DATA_DIR+'truth_train.jsonl', 'rb').read().replace('\\\"', '\"').split('\\n')[:-1]\n",
    "train_Y = map(json.loads, truth_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Number of NNP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nnp_num = lambda x: sum(map(lambda y: y[1] == 'NNP', pos_tag(word_tokenize(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(nnp_num(train_X[0]['targetTitle']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Readability of Target Paragraphs**\n",
    "\n",
    "TextStat package provides us with a huge bulk of useful metrics we can try.\n",
    "\n",
    "Source: https://github.com/shivam5992/textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__doc__',\n",
       " '__init__',\n",
       " '__module__',\n",
       " 'automated_readability_index',\n",
       " 'avg_letter_per_word',\n",
       " 'avg_sentence_length',\n",
       " 'avg_sentence_per_word',\n",
       " 'avg_syllables_per_word',\n",
       " 'char_count',\n",
       " 'coleman_liau_index',\n",
       " 'dale_chall_readability_score',\n",
       " 'difficult_words',\n",
       " 'flesch_kincaid_grade',\n",
       " 'flesch_reading_ease',\n",
       " 'gunning_fog',\n",
       " 'lexicon_count',\n",
       " 'linsear_write_formula',\n",
       " 'polysyllabcount',\n",
       " 'sentence_count',\n",
       " 'smog_index',\n",
       " 'syllable_count',\n",
       " 'text_standard']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Other Features to Try Out\n",
    "dir(textstat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.22\n",
      "0\n",
      "9.9\n",
      "12.12\n",
      "11.2\n",
      "10.98\n",
      "7\n",
      "8.5\n",
      "8.8\n",
      "8th and 9th grade\n"
     ]
    }
   ],
   "source": [
    "print(textstat.flesch_reading_ease(train_X[0]['targetTitle']))\n",
    "print(textstat.smog_index(train_X[0]['targetTitle']))\n",
    "print(textstat.flesch_kincaid_grade(train_X[0]['targetTitle']))\n",
    "print(textstat.coleman_liau_index(train_X[0]['targetTitle']))\n",
    "print(textstat.automated_readability_index(train_X[0]['targetTitle']))\n",
    "print(textstat.dale_chall_readability_score(train_X[0]['targetTitle']))\n",
    "print(textstat.difficult_words(train_X[0]['targetTitle']))\n",
    "print(textstat.linsear_write_formula(train_X[0]['targetTitle']))\n",
    "print(textstat.gunning_fog(train_X[0]['targetTitle']))\n",
    "print(textstat.text_standard(train_X[0]['targetTitle'])) # TODO: Covert this to numerical rep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Number of Tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_tokenize(train_X[0]['targetTitle']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Word Length of Post Text**\n",
    "\n",
    "(Proposed Alternative: Word Length of Target Title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wlen_title = lambda text: len(filter(lambda x: x.isalpha(), word_tokenize(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_len(train_X[0]['targetTitle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. POS 2-gram NNP NNP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# General POS_2GRAM POS Identifier Function\n",
    "def pos_2gram(title, p1, p2):\n",
    "    pos_list = pos_tag(word_tokenize(title))\n",
    "    return sum(map(lambda x: x[0][1] == p1 and x[1][1] == p2, zip(pos_list[:-1], pos_list[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_2gram(train_X[2]['targetTitle'], 'NNP', 'NNP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Whether Post Starts With Number**\n",
    "\n",
    "Here we use either numerical numbers or text based numbers to account for those possibilities.\n",
    "\n",
    "We use a pre-written library to help convert text to numbers and check if they are in fact numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_numeric(text):\n",
    "    try: return type(text2num(text)) == type(0)\n",
    "    except Exception as e: return False\n",
    "    \n",
    "num_start = lambda x: x[0].isdigit() or is_numeric(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(num_start(train_X[2]['targetTitle']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Average Length of Words in Post**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_wordlen(text):\n",
    "    word_lens = map(lambda x: map(lambda y: len(y), word_tokenize(x)), text)\n",
    "    return np.mean(list(itertools.chain.from_iterable(word_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.2788339670468947"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_wordlen(train_X[0]['targetParagraphs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Number of IN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "in_num = lambda t: sum(map(lambda x: x[1] == 'IN', pos_tag(word_tokenize(t))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_num(train_X[2]['targetTitle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. POS 2-gram NNP VBZ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_2gram(train_X[0]['targetTitle'], 'NNP', 'VBZ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. POS 2-gram IN NNP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_2gram(train_X[2]['targetTitle'], 'IN', 'NNP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11. Length of the Longest Word in Post Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_wordlen(text):\n",
    "    word_lens = map(lambda x: map(lambda y: len(y), word_tokenize(x)), text)\n",
    "    return np.max(list(itertools.chain.from_iterable(word_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_wordlen(train_X[0]['postText'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12. Number of WRB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wrb_num = lambda t: sum(map(lambda x: x[1] == 'WRB', pos_tag(word_tokenize(t))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrb_num(train_X[0]['targetTitle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**13. Count POS Pattern WRB** (Potential redundancy with 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**14. Number of NN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nnp_num = lambda t: sum(map(lambda x: x.pos_ == 'NOUN', nlp(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnp_num(train_X[0]['targetTitle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**15. Count POS Pattern NN** (Potential redundancy with 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**16. Whether post text start with 5W1H**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wh_start = lambda t: nlp(t)[0].lower_ in ['who', 'what', 'why', 'where', 'when', 'how']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wh_start(train_X[0]['targetTitle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**17. Whehter exist QM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qm_exist = lambda t: sum(map(lambda x: str(x) == '?', word_tokenize(t))) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qm_exist(train_X[0]['targetTitle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**18. Similarity Between Post and Target Title**\n",
    "\n",
    "We utilized a cosine-similarity based approach based on pretrained word vectors from spaCy rather than the one reported by the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avg_sim(title, body):\n",
    "    title = nlp(title)\n",
    "    return np.mean(map(lambda x: title.similarity(nlp(x)), body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81674047558606366"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_sim(train_X[0]['targetTitle'], train_X[0]['targetParagraphs'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
