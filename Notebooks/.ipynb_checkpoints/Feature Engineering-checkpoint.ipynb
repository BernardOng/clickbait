{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import json\n",
    "import spacy\n",
    "import string\n",
    "import itertools\n",
    "import numpy as np\n",
    "from text2num import text2num\n",
    "from textstat.textstat import textstat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "The following notebook will focus on generating as much hand-crafted features as possible through various methods and techniques. This is simply a workbech we will be using to define each of the function. In the end we hope to parallelize this process to increase the processing speed of this preprocessing phase.\n",
    "\n",
    "We will follow through this also by performing an analysis on a corresponding feature importance analysis to evaluate whether to utilize them or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Application Directory Constants\n",
    "DATA_DIR = '../Data/dataset/'\n",
    "\n",
    "# Load Dataset\n",
    "instance_raw = open(DATA_DIR+'instances_train.jsonl', 'rb').read().replace('\\\"', '\"').split('\\n')[:-1]\n",
    "train_X = map(json.loads, instance_raw)\n",
    "\n",
    "truth_raw = open(DATA_DIR+'truth_train.jsonl', 'rb').read().replace('\\\"', '\"').split('\\n')[:-1]\n",
    "train_Y = map(json.loads, truth_raw)\n",
    "\n",
    "# Load spaCy Object\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Number of NNP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnp_num = lambda t: sum(map(lambda x: x.pos_ == 'PROPN', nlp(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(nnp_num(train_X[0]['targetTitle']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Readability of Target Paragraphs**\n",
    "\n",
    "TextStat package provides us with a huge bulk of useful metrics we can try.\n",
    "\n",
    "Source: https://github.com/shivam5992/textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.22\n",
      "0\n",
      "9.9\n",
      "12.12\n",
      "11.2\n",
      "10.98\n",
      "7\n",
      "8.5\n",
      "8.8\n",
      "8th and 9th grade\n"
     ]
    }
   ],
   "source": [
    "print(textstat.flesch_reading_ease(train_X[0]['targetTitle']))\n",
    "print(textstat.smog_index(train_X[0]['targetTitle']))\n",
    "print(textstat.flesch_kincaid_grade(train_X[0]['targetTitle']))\n",
    "print(textstat.coleman_liau_index(train_X[0]['targetTitle']))\n",
    "print(textstat.automated_readability_index(train_X[0]['targetTitle']))\n",
    "print(textstat.dale_chall_readability_score(train_X[0]['targetTitle']))\n",
    "print(textstat.difficult_words(train_X[0]['targetTitle']))\n",
    "print(textstat.linsear_write_formula(train_X[0]['targetTitle']))\n",
    "print(textstat.gunning_fog(train_X[0]['targetTitle']))\n",
    "print(textstat.text_standard(train_X[0]['targetTitle'])) # TODO: Covert this to numerical rep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Number of Tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp(train_X[0]['targetTitle']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Word Length of Post Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "861"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(map(lambda x: len(nlp(x)), train_X[0]['targetParagraphs']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. POS 2-gram NNP NNP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos2_nnp_nnp(title):\n",
    "    pos_list = [i.pos_ for i in nlp(title)]\n",
    "    return sum(map(lambda x: x[0] == x[1] and x[0] == 'PROPN', zip(pos_list[:-1], pos_list[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos2_nnp_nnp(train_X[2]['targetTitle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Whether Post Starts With Number**\n",
    "\n",
    "Here we use either numerical numbers or text based numbers to account for those possibilities.\n",
    "\n",
    "We use a pre-written library to help convert text to numbers and check if they are in fact numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_numeric(text):\n",
    "    try: return type(text2num(text)) == type(0)\n",
    "    except Exception as e: return False\n",
    "    \n",
    "num_start = lambda x: x[0].isdigit() or is_numeric(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_start(train_X[2]['targetTitle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Average Length of Words in Post**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_wordlen(text):\n",
    "    word_lens = map(lambda x: map(lambda y: len(y), nlp(x)), text)\n",
    "    return np.mean(list(itertools.chain.from_iterable(word_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9477351916376309"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_wordlen(train_X[0]['targetParagraphs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Number of IN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_num = lambda t: sum(map(lambda x: x.pos_ == 'ADP', nlp(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_num(train_X[0]['targetTitle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. POS 2-gram NNP VBZ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pos2_nnp_vbz(title):\n",
    "    pos_list = [i.pos_ for i in nlp(title)]\n",
    "    return sum(map(lambda x: x[0] == 'PROPN' and x[1] == 'VERB', zip(pos_list[:-1], pos_list[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos2_nnp_vbz(train_X[0]['targetTitle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. POS 2-gram IN NNP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pos2_in_nnp(title):\n",
    "    pos_list = [i.pos_ for i in nlp(title)]\n",
    "    return sum(map(lambda x: x[0] == 'ADP' and x[1] == 'PROPN', zip(pos_list[:-1], pos_list[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos2_in_nnp(train_X[0]['targetTitle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11. Length of the Longest Word in Post Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_wordlen(text):\n",
    "    word_lens = map(lambda x: map(lambda y: len(y), nlp(x)), text)\n",
    "    return np.max(list(itertools.chain.from_iterable(word_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_wordlen(train_X[0]['postText'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12. Number of WRB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wrb_num = lambda t: sum(map(lambda x: x.pos_ == 'ADV', nlp(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrb_num(train_X[0]['targetTitle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**13. Count POS Pattern WRB** (Potential redundancy with 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**14. Number of NN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nnp_num = lambda t: sum(map(lambda x: x.pos_ == 'NOUN', nlp(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnp_num(train_X[0]['targetTitle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**15. Count POS Pattern NN** (Potential redundancy with 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**16. Whether post text start with 5W1H**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_start = lambda t: nlp(t)[0].lower_ in ['who', 'what', 'why', 'where', 'when', 'how']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wh_start(train_X[0]['targetTitle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**17. Whehter exist QM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "qm_exist = lambda t: sum(map(lambda x: str(x) == '?', nlp(t))) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qm_exist(train_X[0]['targetTitle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**18. Similarity Between Post and Target Title**\n",
    "\n",
    "We utilized a cosine-similarity based approach based on pretrained word vectors from spaCy rather than the one reported by the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avg_sim(title, body):\n",
    "    title = nlp(title)\n",
    "    return np.mean(map(lambda x: title.similarity(nlp(x)), body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81674047558606366"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_sim(train_X[0]['targetTitle'], train_X[0]['targetParagraphs'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
