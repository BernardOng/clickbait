{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import json\n",
    "import string\n",
    "import operator\n",
    "import itertools\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from pprint import pprint\n",
    "from nltk import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Feature Exploration and Analysis\n",
    "The following notebook performs general exploration of the dataset to find potential correlations and patterns within the dataset itself. \n",
    "\n",
    "This will be used to guide the development of the modeling process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Dataset Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Application Directory Constants\n",
    "DATA_DIR = '../Data/dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "instance_raw = open(DATA_DIR+'instances_train.jsonl', 'rb').read().replace('\\\"', '\"').split('\\n')[:-1]\n",
    "data_train_X = map(json.loads, instance_raw)\n",
    "\n",
    "truth_raw = open(DATA_DIR+'truth_train.jsonl', 'rb').read().replace('\\\"', '\"').split('\\n')[:-1]\n",
    "data_train_Y = map(json.loads, truth_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL RECORDS: 17581\n",
      "TRAIN X - DICTIONARY SCHEMA: [u'postText', u'targetCaptions', u'postMedia', u'targetKeywords', u'targetParagraphs', u'postTimestamp', u'targetTitle', u'id', u'targetDescription']\n",
      "TRAIN Y - DICTIONARY SCHEMA: [u'truthMedian', u'truthClass', u'truthJudgments', u'truthMean', u'truthMode', u'id']\n"
     ]
    }
   ],
   "source": [
    "print('TOTAL RECORDS: ' + str(len(data_train_X)))\n",
    "print('TRAIN X - DICTIONARY SCHEMA: ' + str(data_train_X[0].keys()))\n",
    "print('TRAIN Y - DICTIONARY SCHEMA: ' + str(data_train_Y[0].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'id': u'0',\n",
      " u'postMedia': [],\n",
      " u'postText': [u\"Apple's iOS 9 'App thinning' feature will give your phone's storage a boost\"],\n",
      " u'postTimestamp': u'Tue Jun 09 16:31:10 +0000 2015',\n",
      " u'targetCaptions': [u\"'App thinning' will be supported on Apple's iOS 9 and later models. It ensures apps use the lowest amount of storage space on a device by only downloading the parts it needs to run on individual handsets. It 'slices' the app into 'app variants' that only need to access the specific files on that specific device\",\n",
      "                     u\"'App thinning' will be supported on Apple's iOS 9 and later models. It ensures apps use the lowest amount of storage space on a device by only downloading the parts it needs to run on individual handsets. It 'slices' the app into 'app variants' that only need to access the specific files on that specific device\",\n",
      "                     u\"The guidelines also discuss so-called 'on-demand resources.' This allows developers to omit features from an app until they are opened or requested by the user. The App Store hosts these resources on Apple servers and manages the downloads for the developer and user.\\xa0This will also increase how quickly an app downloads\",\n",
      "                     u\"The guidelines also discuss so-called 'on-demand resources.' This allows developers to omit features from an app until they are opened or requested by the user. The App Store hosts these resources on Apple servers and manages the downloads for the developer and user.\\xa0This will also increase how quickly an app downloads\",\n",
      "                     u\"Apple said it will then 'purge on-demand resources when they are no longer needed and disk space is low' (Apple's storage menu is pictured)\",\n",
      "                     u\"Apple said it will then 'purge on-demand resources when they are no longer needed and disk space is low' (Apple's storage menu is pictured)\",\n",
      "                     u'A 64GB Apple iPhone 6 is typically left with 56GB of free space after pre-installed apps, system files and software is included. A drop of 8GB, leaving 87.5 % of storage free.\\xa0Previous handsets, including the Samsung Galaxy S4 and Apple iPhone 5C typically ranged from between 54% and 79% of free space (illustrated)',\n",
      "                     u'A 64GB Apple iPhone 6 is typically left with 56GB of free space after pre-installed apps, system files and software is included. A drop of 8GB, leaving 87.5 % of storage free.\\xa0Previous handsets, including the Samsung Galaxy S4 and Apple iPhone 5C typically ranged from between 54% and 79% of free space (illustrated)',\n",
      "                     u\"Earlier this year, a pair of disgruntled Apple users filed a lawsuit in Miami accusing the tech giant of 'concealing, omitting and failing to disclose' that on 16GB versions of iPhones, more than 20% of the advertised space isn't available. This graph reveals the capacity available and unavailable to the user\",\n",
      "                     u\"Earlier this year, a pair of disgruntled Apple users filed a lawsuit in Miami accusing the tech giant of 'concealing, omitting and failing to disclose' that on 16GB versions of iPhones, more than 20% of the advertised space isn't available. This graph reveals the capacity available and unavailable to the user\"],\n",
      " u'targetDescription': u\"'App thinning' will be supported on Apple's iOS 9 and later models. It ensures apps use the lowest amount of storage space by 'slicing' it to work on individual handsets (illustrated).\",\n",
      " u'targetKeywords': u'Apple,gives,gigabytes,iOS,9,app,thinning,feature,finally,phone,s,storage,boost',\n",
      " u'targetParagraphs': [u\"Paying for a 64GB phone only to discover that this is significantly reduced by system files and bloatware is the bane of many smartphone owner's lives.\\xa0\",\n",
      "                       u'And the issue became so serious earlier this year that some Apple users even sued the company over it.\\xa0',\n",
      "                       u\"But with the launch of iOS 9, Apple is hoping to address storage concerns by introducing a feature known as 'app thinning.'\",\n",
      "                       u'It has been explained on the watchOS Developer Library site and is aimed at developers looking to optimise their apps to work on iOS and the watchOS.\\xa0',\n",
      "                       u'It ensures apps use the lowest amount of storage space on a device by only downloading the parts it needs run on the particular handset it is being installed onto.',\n",
      "                       u\"It 'slices' the app into 'app variants' that only need to access the specific files on that specific handset.\\xa0\",\n",
      "                       u\"XperiaBlog recently spotted that the 8GB version of Sony's mid-range M4 Aqua has just 1.26GB of space for users.\\xa0\",\n",
      "                       u'This means that firmware, pre-installed apps and Android software take up a staggering 84.25 per cent.\\xa0',\n",
      "                       u\"Sony does let users increase storage space using a microSD card, but as XperiaBlog explained: 'Sony should never have launched an 8GB version of the Xperia M4 Aqua.\\xa0\",\n",
      "                       u\"'If you are thinking about purchasing this model, be aware of what you are buying into.'\",\n",
      "                       u\"Previously, apps would need to be able to run on all handsets and account for the varying files, chipsets and power so contained sections that weren't always relevant to the phone it was being installed on.\",\n",
      "                       u'This made them larger than they needed to be.\\xa0',\n",
      "                       u'Under the new plans, when a phone is downloaded from the App Store, the app recognises which phone it is being installed onto and only pulls in the files and code it needs to work on that particular device.\\xa0',\n",
      "                       u'For iOS, sliced apps are supported on the latest iTunes and on devices running iOS 9.0 and later.\\xa0',\n",
      "                       u\"In all other cases, the App Store will deliver the previous 'universal apps' to customers.\",\n",
      "                       u\"The guidelines also discuss so-called 'on-demand resources.'\\xa0This allows developers to omit features from an app until they are opened or requested by the user.\\xa0\",\n",
      "                       u'The App Store hosts these resources on Apple servers and manages the downloads for the developer and user.\\xa0',\n",
      "                       u'This will also increase how quickly an app downloads.\\xa0',\n",
      "                       u'An example given by Apple is a game app that may divide resources into game levels and request the next level of resources only when the app anticipates the user has completed the previous level.',\n",
      "                       u'Similarly, the app can request In-App Purchase resources only when the user buys a corresponding in-app purchase.',\n",
      "                       u\"Apple explained the operating system will then 'purge on-demand resources when they are no longer needed and disk space is low', removing them until they are needed again.\",\n",
      "                       u'And the whole iOS 9 software has been designed to be thinner during updates, namely from 4.6GB to 1.3GB, to free up space.\\xa0',\n",
      "                       u'This app thinning applies to third-party apps created by developers.\\xa0',\n",
      "                       u\"Apple doesn't say if it will apply to the apps Apple pre-installed on devices, such as Stocks, Weather and Safari - but it is likely that it will in order to make iOS 9 smaller.\\xa0\",\n",
      "                       u'As an example of storage space on Apple devices, a 64GB Apple iPhone 6 is typically left with 56GB of free space after pre-installed apps, system files and software is included.\\xa0',\n",
      "                       u'A drop of 8GB, leaving 87.5 per cent of storage free.\\xa0',\n",
      "                       u\"By comparison, Samsung's 64GB S6 Edge has 53.42GB of available space, and of this 9GB is listed as system memory.\\xa0\",\n",
      "                       u'Although this is a total drop of almost 11GB, it equates to 83 per cent of space free.\\xa0',\n",
      "                       u'By comparison, on a 32GB S6 MailOnline found 23.86GB of space was available, with 6.62GB attributed to system memory.',\n",
      "                       u'This is a drop of just over 8GB and leaves 75 per cent free.',\n",
      "                       u'Samsung said it, too, had addressed complaints about bloatware and storage space with its S6 range. \\xa0',\n",
      "                       u'Previous handsets, including the Samsung Galaxy S4 and Apple iPhone 5C typically ranged from between 54 per cent and 79 per cent of free space.',\n",
      "                       u'\\xa0',\n",
      "                       u\"Businessman 'killed his best friend when he crashed jet-powered dinghy into his \\xa31million yacht while showing off' as his wife filmed them\"],\n",
      " u'targetTitle': u\"Apple gives back gigabytes: iOS 9 'app thinning' feature will finally give your phone's storage a boost\"}\n",
      "\n",
      "{u'id': u'0',\n",
      " u'truthClass': u'no-clickbait',\n",
      " u'truthJudgments': [0.0, 0.6666667, 0.0, 0.33333334, 0.0],\n",
      " u'truthMean': 0.2,\n",
      " u'truthMedian': 0.0,\n",
      " u'truthMode': 0.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Non-Clickbait Example\n",
    "pprint(data_train_X[0])\n",
    "print()\n",
    "\n",
    "pprint(data_train_Y[0])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'id': u'2',\n",
      " u'postMedia': [],\n",
      " u'postText': [u'U.S. Soccer should start answering tough questions about Hope Solo, @eric_adelson writes.'],\n",
      " u'postTimestamp': u'Fri Jun 12 23:36:05 +0000 2015',\n",
      " u'targetCaptions': [u'US to vote for Ali in FIFA election and not Blatter',\n",
      "                     u'US to vote for Ali in FIFA election and not Blatter',\n",
      "                     u\"FILE - This Oct. 10, 2014, file photo shows Sunil Gulati, president of the United States Soccer Federation, during a press conference in Bristol, Conn. The United States says it will vote for Jordan's Prince Ali bin Al-Hussein for FIFA president Friday, May 29, 2015 and not for incumbent Sepp Blatter. (AP Photo/Elise Amendola, File)\"],\n",
      " u'targetDescription': u\"A U.S. Senator's scathing letter questioned U.S. Soccer's inadequate handling of Solo's domestic violence charges. It's time for Sunil Gulati to respond.\",\n",
      " u'targetKeywords': u'',\n",
      " u'targetParagraphs': [u\"WINNIPEG, Manitoba \\u2013 The bubble U.S. Soccer is putting around Hope Solo isn't working to calm anyone's concerns about the star goalkeeper.\",\n",
      "                       u\"The latest lament comes from no less than a U.S. Senator, who into Solo's domestic violence incident of last year and offer a detailed explanation of why Solo is on the field. She is expected to be the starting goalkeeper when the USA plays Sweden in its second group game at the Women's World Cup on Friday.\",\n",
      "                       u'[FC Yahoo: ]',\n",
      "                       u'U.S. Senator Richard Blumenthal of Connecticut penned a lengthy complaint about the near-silence the organization has given on Solo, especially in the wake of ESPN\\'s \"Outside the Lines\" report on Sunday. Blumenthal wrote that if the report is accurate \"U.S. Soccer\\'s approach to domestic violence and family violence is at best superficial and at worst dangerously neglectful and self-serving.\"',\n",
      "                       u'This situation is well beyond Solo now. U.S. Soccer has made this a referendum about its own ability to represent the values of the nation. \"As boys and girls tune into Friday\\'s game, watching the women on TV as role models,\" Blumenthal wrote, \"what is the message of starting Hope Solo at goal?\"',\n",
      "                       u\"[Women's World Cup: | | | ]\",\n",
      "                       u\"U.S. Soccer is not only avoiding difficult questions, it is also avoiding an account of all its actions. Even NFL commissioner Roger Goodell has, to some extent, owned up to his failures on the Ray Rice case, yet Gulati has not even decried Solo's poor decisions.\",\n",
      "                       u\"Last September, three months after Solo's domestic violence charges (which were later ), Gulati released this vague statement on the matter:\",\n",
      "                       u'\"U.S. Soccer takes the issue of domestic violence very seriously. From the beginning, we considered the information available and have taken a deliberate and thoughtful approach regarding Hope Solo\\'s status with the national team. Based on that information, U.S. Soccer stands by our decision to allow her to participate with the team as the legal process unfolds. If new information becomes available we will carefully consider it.\"',\n",
      "                       u\"A lot of this would be solved if Gulati and Solo held a press conference and claimed some accountability. It's clear from Monday's dominant performance in a 3-1 tournament-opening win over Australia that Solo is not distracted by the national discussion of her past, so a short appearance \\u2013 even without reporters' questions \\u2013 probably won't ruin the U.S.'s chances for a trophy. And claiming that Solo has a match to focus on isn't credible as it's basically an admission that a single game is more important than a discussion of domestic violence.\",\n",
      "                       u\"For Gulati, there is little excuse. The silence, the lack of punishment and then the decision to allow head coach Jill Ellis to discuss (or not discuss) the situation here, combines to make the top official of American soccer look like he doesn't prioritize this issue.\",\n",
      "                       u'\"In the wake of this violent incident, U.S. Soccer offered no comment publicly for three months,\" Blumenthal wrote. \"It finally issued a statement that was purportedly the result of a \\'deliberate and thoughtful approach\\' to consider the incident and determine Hope Solo\\'s status with the team, but it neglected to include an effort to contact the alleged victims.\"',\n",
      "                       u'The more U.S. Soccer tries to shift focus to the field, the less it accomplishes that. This is the Super Bowl of women\\'s soccer, and decrying this as \"old news\" doesn\\'t work because the entire country is watching now. Countless Americans are debating whether to root for Solo or not, and her protectors are effectively convincing a lot of people to remain skeptical of her.',\n",
      "                       u\"It doesn't have to be this way. A better explanation of what Gulati has done on this topic \\u2013 and a better explanation of what Solo has done to work on her problems \\u2013 would go a long way toward moving on, especially the way U.S. Soccer clearly wants. Instead, there is opacity where there should be transparency.\",\n",
      "                       u'The NFL has come under a lot of scrutiny for its efforts to \"protect the shield,\" but U.S. Soccer\\'s shield stands for a lot more than just a sport. That shield shouldn\\'t only be used to defend a player.',\n",
      "                       u'735'],\n",
      " u'targetTitle': u'U.S. Soccer should start answering tough questions about Hope Solo'}\n",
      "\n",
      "{u'id': u'2',\n",
      " u'truthClass': u'clickbait',\n",
      " u'truthJudgments': [0.33333334, 0.6666667, 1.0, 0.0, 0.6666667],\n",
      " u'truthMean': 0.53333336,\n",
      " u'truthMedian': 0.6666667,\n",
      " u'truthMode': 0.6666667}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clickbait Example\n",
    "pprint(data_train_X[2])\n",
    "print()\n",
    "\n",
    "pprint(data_train_Y[2])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Class Label Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete Class Counts\n",
      "CLICKBAIT POS: 4433\t0.252147204368\n",
      "CLICKBAIT NEG: 13148\t0.747852795632\n",
      "\n",
      "Median Distribution\n",
      "DescribeResult(nobs=17581, minmax=(0.0, 1.0), mean=0.28597159927869292, variance=0.11023303798566474, skewness=0.8424582531710086, kurtosis=-0.49802822996455554)\n",
      "\n",
      "Mean Distribution\n",
      "DescribeResult(nobs=17581, minmax=(0.0, 1.0), mean=0.32994331090893236, variance=0.063558726054432441, skewness=0.6992524359835718, kurtosis=-0.4036965464625224)\n",
      "\n",
      "Mode Distribution\n",
      "DescribeResult(nobs=17581, minmax=(0.0, 1.0), mean=0.27558159451935615, variance=0.12979563066539043, skewness=0.9663085836264188, kurtosis=-0.5125765548566874)\n",
      "\n",
      "Overall Distribution\n",
      "DescribeResult(nobs=87905, minmax=(0.0, 1.0), mean=0.32994331102621804, variance=0.13623815425012464, skewness=0.6748670366487401, kurtosis=-0.9773744894072753)\n"
     ]
    }
   ],
   "source": [
    "print('Discrete Class Counts')\n",
    "c_list = map(lambda x: 0 if x['truthClass'] == 'no-clickbait' else 1, data_train_Y)\n",
    "print('CLICKBAIT POS: ' + str(sum(c_list)) + '\\t' + str(sum(c_list)/float(len(c_list))))\n",
    "print('CLICKBAIT NEG: ' + str(len(c_list) - sum(c_list)) + '\\t' + str(1 - (sum(c_list)/float(len(c_list)))))\n",
    "print()\n",
    "\n",
    "print('Median Distribution')\n",
    "med_list = map(lambda x: x['truthMedian'], data_train_Y)\n",
    "pprint(stats.describe(med_list))\n",
    "print()\n",
    "\n",
    "print('Mean Distribution')\n",
    "mean_list = map(lambda x: x['truthMean'], data_train_Y)\n",
    "pprint(stats.describe(mean_list))\n",
    "print()\n",
    "\n",
    "print('Mode Distribution')\n",
    "mode_list = map(lambda x: x['truthMode'], data_train_Y)\n",
    "pprint(stats.describe(mode_list))\n",
    "print()\n",
    "\n",
    "print('Overall Distribution')\n",
    "overall_list = map(lambda x: x['truthJudgments'], data_train_Y)\n",
    "overall_list = list(itertools.chain.from_iterable(overall_list))\n",
    "pprint(stats.describe(overall_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Notes and Observations\n",
    "* Dataset is generally biased towards negative samples - for every 3 negative, we have 1 positive.\n",
    "* Distribution of the overall data is generally positively skewed.\n",
    "* Mean seems to be quite consistent against other parameters - indicating most of the news in the data is negatively baity.\n",
    "* Variance is quite small, so the confusion rate of the end model should approach this distribution at the very least.\n",
    "* **TODO**: Identify the articles which are away from the variance and see why it would be controvertial.\n",
    "* **TODO**: Devise strategies for oversampling methods when training the model - bootstrapping, smote, stratified k-fold, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Feature Analysis\n",
    "The following analysis is based on the various features provided by the data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Training Based on Bait or Not-Bait\n",
    "data_train_X_bait = filter(lambda y: y is not '', map(lambda x: x[0] if x[1]['truthClass'] == 'clickbait' else '', zip(data_train_X, data_train_Y)))\n",
    "data_train_X_notbait = filter(lambda y: y is not '', map(lambda x: x[0] if x[1]['truthClass'] == 'no-clickbait' else '', zip(data_train_X, data_train_Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Title Analysis\n",
    "\n",
    "### Target Title Character/Word Count Distributions\n",
    "Analyze the character level distribution details of the title.\n",
    "\n",
    "**TODO:** Perform statistical testing to check significance of the distribution differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_word_dist(input_data):\n",
    "    print('Character Length Distribution')\n",
    "    title_chlen_list = map(lambda x: len(x['targetTitle']), input_data)\n",
    "    pprint(stats.describe(title_chlen_list))\n",
    "    print()\n",
    "\n",
    "    print('Word Length Distribution (Assume Separation by Whitespace)')\n",
    "    title_chlen_list = map(lambda x: map(len, x['targetTitle'].split(' ')), input_data)\n",
    "    title_chlen_list = list(itertools.chain.from_iterable(title_chlen_list))\n",
    "    pprint(stats.describe(title_chlen_list))\n",
    "    print()\n",
    "\n",
    "    print('Upper Case Characters Distribution')\n",
    "    title_cap_list = map(lambda x: sum([1 for i in x['targetTitle'] if i.isupper()]), input_data)\n",
    "    pprint(stats.describe(title_cap_list))\n",
    "    print()\n",
    "\n",
    "    print('Lower Case Characters Distribution')\n",
    "    title_low_list = map(lambda x: sum([1 for i in x['targetTitle'] if i.islower()]), input_data)\n",
    "    pprint(stats.describe(title_low_list))\n",
    "    print()\n",
    "\n",
    "    print('Punctuation Distribution')\n",
    "    title_punct_list = map(lambda x: sum([1 for i in x['targetTitle'] if i in string.punctuation and i is not ' ']), input_data)\n",
    "    pprint(stats.describe(title_punct_list))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entire Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Length Distribution\n",
      "DescribeResult(nobs=17581, minmax=(4, 4038), mean=80.607360218417611, variance=22675.621422502139, skewness=15.056280752353373, kurtosis=268.055872703193)\n",
      "\n",
      "Word Length Distribution (Assume Separation by Whitespace)\n",
      "DescribeResult(nobs=232465, minmax=(0, 31), mean=5.1718495257350572, variance=6.6723698434872043, skewness=0.7323561694237745, kurtosis=0.9709286012182616)\n",
      "\n",
      "Upper Case Characters Distribution\n",
      "DescribeResult(nobs=17581, minmax=(0, 199), mean=7.4869461350321371, variance=97.60456508179692, skewness=10.383947623179736, kurtosis=140.83544204595051)\n",
      "\n",
      "Lower Case Characters Distribution\n",
      "DescribeResult(nobs=17581, minmax=(0, 3096), mean=58.27302201239975, variance=13117.971416060323, skewness=15.129069031202686, kurtosis=272.816959987369)\n",
      "\n",
      "Punctuation Distribution\n",
      "DescribeResult(nobs=17581, minmax=(0, 74), mean=1.6065070246288606, variance=9.4275207961537237, skewness=10.631201733725636, kurtosis=157.1301561576207)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "char_word_dist(data_train_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clickbait Title Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Length Distribution\n",
      "DescribeResult(nobs=4433, minmax=(4, 4038), mean=79.248364538687113, variance=21845.357750596322, skewness=16.706889125188475, kurtosis=330.58211989180535)\n",
      "\n",
      "Word Length Distribution (Assume Separation by Whitespace)\n",
      "DescribeResult(nobs=57667, minmax=(0, 29), mean=5.1688834168588622, variance=6.6759658100650556, skewness=0.7591308305442004, kurtosis=1.0895376951683469)\n",
      "\n",
      "Upper Case Characters Distribution\n",
      "DescribeResult(nobs=4433, minmax=(0, 194), mean=7.4432664110083469, variance=88.180952505047046, skewness=10.689453923145273, kurtosis=153.87172985695204)\n",
      "\n",
      "Lower Case Characters Distribution\n",
      "DescribeResult(nobs=4433, minmax=(0, 3096), mean=57.193097225355288, variance=12714.446008297627, skewness=16.78385777183443, kurtosis=335.1615601802185)\n",
      "\n",
      "Punctuation Distribution\n",
      "DescribeResult(nobs=4433, minmax=(0, 64), mean=1.5851567787051657, variance=8.8240305315972005, skewness=11.18981279316539, kurtosis=177.6721286470377)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "char_word_dist(data_train_X_bait)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Clickbait Title Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Length Distribution\n",
      "DescribeResult(nobs=13148, minmax=(4, 4038), mean=81.065561302099184, variance=22956.404615970001, skewness=14.5367689965689, kurtosis=248.84125546177148)\n",
      "\n",
      "Word Length Distribution (Assume Separation by Whitespace)\n",
      "DescribeResult(nobs=174798, minmax=(0, 31), mean=5.1728280643943299, variance=6.6712178365455204, skewness=0.7235151550621862, kurtosis=0.9317997014531723)\n",
      "\n",
      "Upper Case Characters Distribution\n",
      "DescribeResult(nobs=13148, minmax=(0, 199), mean=7.5016732582902339, variance=100.78793361134231, skewness=10.283432372583922, kurtosis=136.89659270240287)\n",
      "\n",
      "Lower Case Characters Distribution\n",
      "DescribeResult(nobs=13148, minmax=(0, 3096), mean=58.637131122604195, variance=13254.476287562635, skewness=14.603099399783527, kurtosis=253.4003772389118)\n",
      "\n",
      "Punctuation Distribution\n",
      "DescribeResult(nobs=13148, minmax=(0, 74), mean=1.6137055065409187, variance=9.631475642178545, skewness=10.458613154710031, kurtosis=151.0343731438817)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "char_word_dist(data_train_X_notbait)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Title Word Frequency Distribution\n",
    "Analyze the most frequently used words in the entire corpus set.\n",
    "\n",
    "Objective here is to try to find the most 'baity' words as possible - good for crafting a 'bait' corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Text\n",
    "tkn = RegexpTokenizer(r'\\w+')\n",
    "def preprocess(title):\n",
    "    title = word_tokenize(title.lower())  # Tokenize & Normalize Text\n",
    "    title = set(title) - set(string.punctuation)  # Removes Punctuation (WARNING: This messes up the order of text; doesn't matter for now)\n",
    "    title = set(title) - set(stopwords.words('english'))  # Removes Common Stopwords\n",
    "    # title = set(title) - set(['\\'s']) # Remove \"'s\" - for some reason it's not removed...\n",
    "    return title\n",
    "\n",
    "def token_freq_dist(titles, k=10):\n",
    "    token_X_list = map(lambda x: preprocess(x['targetTitle']), titles)\n",
    "    token_X_list = list(itertools.chain.from_iterable(token_X_list))\n",
    "    token_X_dist = sorted(Counter(token_X_list).items(), key=operator.itemgetter(1), reverse=True)\n",
    "    pprint(token_X_dist[0:k])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entire Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u\"'s\", 2617),\n",
      " (u'\\u2019', 2150),\n",
      " (u'trump', 2069),\n",
      " (u'new', 1002),\n",
      " (u'\\u2018', 871),\n",
      " (u'says', 689),\n",
      " (u'video', 497),\n",
      " (u'2017', 488),\n",
      " (u'donald', 460),\n",
      " (u'us', 452),\n",
      " (u\"n't\", 425),\n",
      " (u'first', 404),\n",
      " (u'man', 391),\n",
      " (u'world', 376),\n",
      " (u'people', 376),\n",
      " (u'house', 354),\n",
      " (u'news', 340),\n",
      " (u'woman', 314),\n",
      " (u'women', 307),\n",
      " (u'live', 302),\n",
      " (u'white', 295),\n",
      " (u'time', 294),\n",
      " (u'2016', 292),\n",
      " (u'president', 292),\n",
      " (u'police', 289),\n",
      " (u'one', 284),\n",
      " (u'could', 282),\n",
      " (u'obama', 277),\n",
      " (u'may', 277),\n",
      " (u'china', 271)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "token_freq_dist(data_train_X, k=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clickbait Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u\"'s\", 650),\n",
      " (u'\\u2019', 547),\n",
      " (u'trump', 493),\n",
      " (u'new', 219),\n",
      " (u'\\u2018', 201),\n",
      " (u'says', 176),\n",
      " (u'video', 122),\n",
      " (u'2017', 120),\n",
      " (u'us', 115),\n",
      " (u\"n't\", 112),\n",
      " (u'world', 109),\n",
      " (u'people', 107),\n",
      " (u'first', 102),\n",
      " (u'man', 101),\n",
      " (u'donald', 91),\n",
      " (u'house', 89),\n",
      " (u'news', 82),\n",
      " (u'white', 82),\n",
      " (u'2016', 78),\n",
      " (u'woman', 77),\n",
      " (u'women', 74),\n",
      " (u'president', 73),\n",
      " (u'watch', 72),\n",
      " (u'time', 71),\n",
      " (u'obama', 71),\n",
      " (u'one', 69),\n",
      " (u'could', 69),\n",
      " (u'top', 68),\n",
      " (u'life', 65),\n",
      " (u'police', 65)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "token_freq_dist(data_train_X_bait, k=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Clickbait Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u\"'s\", 1967),\n",
      " (u'\\u2019', 1603),\n",
      " (u'trump', 1576),\n",
      " (u'new', 783),\n",
      " (u'\\u2018', 670),\n",
      " (u'says', 513),\n",
      " (u'video', 375),\n",
      " (u'donald', 369),\n",
      " (u'2017', 368),\n",
      " (u'us', 337),\n",
      " (u\"n't\", 313),\n",
      " (u'first', 302),\n",
      " (u'man', 290),\n",
      " (u'people', 269),\n",
      " (u'world', 267),\n",
      " (u'house', 265),\n",
      " (u'news', 258),\n",
      " (u'live', 239),\n",
      " (u'woman', 237),\n",
      " (u'women', 233),\n",
      " (u'police', 224),\n",
      " (u'time', 223),\n",
      " (u'president', 219),\n",
      " (u'one', 215),\n",
      " (u'2016', 214),\n",
      " (u'could', 213),\n",
      " (u'white', 213),\n",
      " (u'may', 212),\n",
      " (u'china', 209),\n",
      " (u'obama', 206)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "token_freq_dist(data_train_X_notbait, k=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Text Analysis\n",
    "The following analysis performs analytics over the data from the actual content of the Tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Post Character/Word Count Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def char_word_dist(input_data):\n",
    "    print('Character Length Distribution')\n",
    "    title_chlen_list = map(lambda x: len(' '.join(x['postText'])), input_data)\n",
    "    pprint(stats.describe(title_chlen_list))\n",
    "    print()\n",
    "\n",
    "    print('Word Length Distribution (Assume Separation by Whitespace)')\n",
    "    title_chlen_list = map(lambda x: map(len, ' '.join(x['postText']).split(' ')), input_data)\n",
    "    title_chlen_list = list(itertools.chain.from_iterable(title_chlen_list))\n",
    "    pprint(stats.describe(title_chlen_list))\n",
    "    print()\n",
    "\n",
    "    print('Upper Case Characters Distribution')\n",
    "    title_cap_list = map(lambda x: sum([1 for i in ' '.join(x['postText']) if i.isupper()]), input_data)\n",
    "    pprint(stats.describe(title_cap_list))\n",
    "    print()\n",
    "\n",
    "    print('Lower Case Characters Distribution')\n",
    "    title_low_list = map(lambda x: sum([1 for i in ' '.join(x['postText']) if i.islower()]), input_data)\n",
    "    pprint(stats.describe(title_low_list))\n",
    "    print()\n",
    "\n",
    "    print('Punctuation Distribution')\n",
    "    title_punct_list = map(lambda x: sum([1 for i in ' '.join(x['postText']) if i in string.punctuation and i is not ' ']), input_data)\n",
    "    pprint(stats.describe(title_punct_list))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entire Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Length Distribution\n",
      "DescribeResult(nobs=17581, minmax=(0, 143), mean=71.513907058756615, variance=472.03150453493367, skewness=-0.2530411285578084, kurtosis=0.548535346887689)\n",
      "\n",
      "Word Length Distribution (Assume Separation by Whitespace)\n",
      "DescribeResult(nobs=206198, minmax=(0, 107), mean=5.1827321312524859, variance=7.6507700695283516, skewness=1.3463019105108947, kurtosis=12.193234079462783)\n",
      "\n",
      "Upper Case Characters Distribution\n",
      "DescribeResult(nobs=17581, minmax=(0, 40), mean=3.9819122916785168, variance=9.6353952280292248, skewness=1.8560016831070376, kurtosis=6.144072662794134)\n",
      "\n",
      "Lower Case Characters Distribution\n",
      "DescribeResult(nobs=17581, minmax=(0, 108), mean=53.955861441328707, variance=287.44424052778561, skewness=-0.26173039093673234, kurtosis=0.4969114777660284)\n",
      "\n",
      "Punctuation Distribution\n",
      "DescribeResult(nobs=17581, minmax=(0, 100), mean=1.9985211307661681, variance=4.2937407024687104, skewness=7.256441834179071, kurtosis=286.8242625948423)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "char_word_dist(data_train_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clickbait Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Length Distribution\n",
      "DescribeResult(nobs=4433, minmax=(0, 139), mean=70.37942702458831, variance=481.0703528304698, skewness=-0.25896458717108306, kurtosis=0.49055150702123385)\n",
      "\n",
      "Word Length Distribution (Assume Separation by Whitespace)\n",
      "DescribeResult(nobs=51266, minmax=(0, 107), mean=5.1722194046736627, variance=7.7780644665197745, skewness=1.9682594999608918, kurtosis=36.418490392353306)\n",
      "\n",
      "Upper Case Characters Distribution\n",
      "DescribeResult(nobs=4433, minmax=(0, 24), mean=3.7769005188360025, variance=8.5321197231788819, skewness=1.7029072924633517, kurtosis=4.5521407296839715)\n",
      "\n",
      "Lower Case Characters Distribution\n",
      "DescribeResult(nobs=4433, minmax=(0, 106), mean=53.216106474170992, variance=294.69967734606138, skewness=-0.2625641700090724, kurtosis=0.48109122179964503)\n",
      "\n",
      "Punctuation Distribution\n",
      "DescribeResult(nobs=4433, minmax=(0, 100), mean=1.977667493796526, variance=5.7194921213641372, skewness=16.25141985154566, kurtosis=636.5186559948596)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "char_word_dist(data_train_X_bait)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Clickbait Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Length Distribution\n",
      "DescribeResult(nobs=13148, minmax=(0, 143), mean=71.896410100395499, variance=468.44001753683267, skewness=-0.24995343207014578, kurtosis=0.568397188117745)\n",
      "\n",
      "Word Length Distribution (Assume Separation by Whitespace)\n",
      "DescribeResult(nobs=154932, minmax=(0, 66), mean=5.186210724705032, variance=7.6086504353051359, skewness=1.133627919877439, kurtosis=3.8193596028782926)\n",
      "\n",
      "Upper Case Characters Distribution\n",
      "DescribeResult(nobs=13148, minmax=(0, 40), mean=4.0510343778521447, variance=9.9891044293345406, skewness=1.8885664647086793, kurtosis=6.462052710872953)\n",
      "\n",
      "Lower Case Characters Distribution\n",
      "DescribeResult(nobs=13148, minmax=(0, 108), mean=54.20527836933374, variance=284.77348092197212, skewness=-0.26018270421688167, kurtosis=0.5013843344248716)\n",
      "\n",
      "Punctuation Distribution\n",
      "DescribeResult(nobs=13148, minmax=(0, 20), mean=2.0055521752357772, variance=3.813234554743119, skewness=1.5363956815581272, kurtosis=3.8486335353845815)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "char_word_dist(data_train_X_notbait)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Post Word Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_freq_dist(titles, k=10):\n",
    "    token_X_list = map(lambda x: preprocess(' '.join(x['postText'])), titles)\n",
    "    token_X_list = list(itertools.chain.from_iterable(token_X_list))\n",
    "    token_X_dist = sorted(Counter(token_X_list).items(), key=operator.itemgetter(1), reverse=True)\n",
    "    pprint(token_X_dist[0:k])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entire Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u\"'s\", 2910),\n",
      " (u'trump', 1709),\n",
      " (u'\\u2019', 1393),\n",
      " (u'``', 1379),\n",
      " (u\"''\", 1371),\n",
      " (u'new', 951),\n",
      " (u'says', 614),\n",
      " (u'via', 524),\n",
      " (u\"n't\", 494),\n",
      " (u'rt', 450),\n",
      " (u'people', 437),\n",
      " (u'president', 380),\n",
      " (u'one', 374),\n",
      " (u'first', 368),\n",
      " (u'donald', 360),\n",
      " (u'world', 349),\n",
      " (u'us', 338),\n",
      " (u'could', 320),\n",
      " (u'man', 310),\n",
      " (u'house', 288),\n",
      " (u'day', 286),\n",
      " (u'year', 285),\n",
      " (u'may', 284),\n",
      " (u'say', 283),\n",
      " (u'police', 280),\n",
      " (u'watch', 277),\n",
      " (u'video', 274),\n",
      " (u'best', 269),\n",
      " (u'get', 267),\n",
      " (u'u.s.', 260)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "token_freq_dist(data_train_X, k=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clickbait Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u\"'s\", 739),\n",
      " (u'trump', 406),\n",
      " (u\"''\", 354),\n",
      " (u'``', 352),\n",
      " (u'\\u2019', 340),\n",
      " (u'new', 224),\n",
      " (u'says', 155),\n",
      " (u\"n't\", 131),\n",
      " (u'via', 126),\n",
      " (u'rt', 113),\n",
      " (u'people', 111),\n",
      " (u'president', 104),\n",
      " (u'one', 98),\n",
      " (u'first', 93),\n",
      " (u'world', 91),\n",
      " (u'us', 91),\n",
      " (u'man', 89),\n",
      " (u'could', 83),\n",
      " (u'donald', 81),\n",
      " (u'get', 76),\n",
      " (u'year', 74),\n",
      " (u'may', 73),\n",
      " (u'day', 71),\n",
      " (u'best', 69),\n",
      " (u'house', 69),\n",
      " (u'watch', 67),\n",
      " (u'life', 65),\n",
      " (u'video', 65),\n",
      " (u'years', 64),\n",
      " (u'say', 64)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "token_freq_dist(data_train_X_bait, k=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Clickbait Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u\"'s\", 2171),\n",
      " (u'trump', 1303),\n",
      " (u'\\u2019', 1053),\n",
      " (u'``', 1027),\n",
      " (u\"''\", 1017),\n",
      " (u'new', 727),\n",
      " (u'says', 459),\n",
      " (u'via', 398),\n",
      " (u\"n't\", 363),\n",
      " (u'rt', 337),\n",
      " (u'people', 326),\n",
      " (u'donald', 279),\n",
      " (u'one', 276),\n",
      " (u'president', 276),\n",
      " (u'first', 275),\n",
      " (u'world', 258),\n",
      " (u'us', 247),\n",
      " (u'could', 237),\n",
      " (u'police', 222),\n",
      " (u'man', 221),\n",
      " (u'say', 219),\n",
      " (u'house', 219),\n",
      " (u'day', 215),\n",
      " (u'year', 211),\n",
      " (u'may', 211),\n",
      " (u'u.s.', 210),\n",
      " (u'watch', 210),\n",
      " (u'video', 209),\n",
      " (u'best', 200),\n",
      " (u'women', 198)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "token_freq_dist(data_train_X_notbait, k=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Keyword Frequency Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_freq_dist(titles, k=10):\n",
    "    keywords_X_list = map(lambda x: preprocess(x['targetKeywords']) if len(x['targetKeywords']) is not 0 else [], titles)\n",
    "    keywords_X_list = list(itertools.chain.from_iterable(keywords_X_list))\n",
    "    keywords_X_dist = sorted(Counter(keywords_X_list).items(), key=operator.itemgetter(1), reverse=True)\n",
    "    pprint(keywords_X_dist[0:k])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entire Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'news', 3436),\n",
      " (u'trump', 1639),\n",
      " (u'donald', 1265),\n",
      " (u'politics', 1242),\n",
      " (u'world', 991),\n",
      " (u'general', 728),\n",
      " (u'political', 728),\n",
      " (u'government', 689),\n",
      " (u'us', 674),\n",
      " (u'national', 621),\n",
      " (u'uk', 560),\n",
      " (u'international', 503),\n",
      " (u'standard', 493),\n",
      " (u'health', 467),\n",
      " (u'entertainment', 462),\n",
      " (u'new', 452),\n",
      " (u'united', 441),\n",
      " (u'breaking', 432),\n",
      " (u'big', 378),\n",
      " (u\"'s\", 370),\n",
      " (u'business', 359),\n",
      " (u'election', 345),\n",
      " (u'states', 341),\n",
      " (u'culture', 335),\n",
      " (u'cnn.com', 322),\n",
      " (u'football', 314),\n",
      " (u'breitbart', 298),\n",
      " (u'nba', 294),\n",
      " (u'relations', 290),\n",
      " (u'nfl', 280)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "token_freq_dist(data_train_X, k=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clickbait Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'news', 806),\n",
      " (u'trump', 374),\n",
      " (u'donald', 293),\n",
      " (u'politics', 284),\n",
      " (u'world', 236),\n",
      " (u'government', 167),\n",
      " (u'general', 165),\n",
      " (u'political', 158),\n",
      " (u'uk', 145),\n",
      " (u'us', 139),\n",
      " (u'national', 136),\n",
      " (u'standard', 129),\n",
      " (u'health', 127),\n",
      " (u'international', 112),\n",
      " (u'business', 108),\n",
      " (u'new', 107),\n",
      " (u'entertainment', 106),\n",
      " (u'united', 96),\n",
      " (u'culture', 94),\n",
      " (u'big', 94),\n",
      " (u'breaking', 93),\n",
      " (u\"'s\", 92),\n",
      " (u'breitbart', 83),\n",
      " (u'tv', 80),\n",
      " (u'industrial', 77),\n",
      " (u'states', 75),\n",
      " (u'nfl', 75),\n",
      " (u'football', 72),\n",
      " (u'presidential', 71),\n",
      " (u'election', 70)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "token_freq_dist(data_train_X_bait, k=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Clickbait Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'news', 2630),\n",
      " (u'trump', 1265),\n",
      " (u'donald', 972),\n",
      " (u'politics', 958),\n",
      " (u'world', 755),\n",
      " (u'political', 570),\n",
      " (u'general', 563),\n",
      " (u'us', 535),\n",
      " (u'government', 522),\n",
      " (u'national', 485),\n",
      " (u'uk', 415),\n",
      " (u'international', 391),\n",
      " (u'standard', 364),\n",
      " (u'entertainment', 356),\n",
      " (u'new', 345),\n",
      " (u'united', 345),\n",
      " (u'health', 340),\n",
      " (u'breaking', 339),\n",
      " (u'big', 284),\n",
      " (u\"'s\", 278),\n",
      " (u'election', 275),\n",
      " (u'states', 266),\n",
      " (u'cnn.com', 258),\n",
      " (u'business', 251),\n",
      " (u'football', 242),\n",
      " (u'culture', 241),\n",
      " (u'nba', 229),\n",
      " (u'north', 224),\n",
      " (u'relations', 221),\n",
      " (u'breitbart', 215)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "token_freq_dist(data_train_X_notbait, k=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Timestamp Analysis \n",
    "Analyzing timestamp distribution on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp(input_data):\n",
    "    # Analyze Days of Week\n",
    "    time_days_list = map(lambda x: x['postTimestamp'].split(' ')[0], input_data)\n",
    "    time_days_dist = sorted(Counter(time_days_list).items(), key=operator.itemgetter(1), reverse=True)\n",
    "    pprint(time_days_dist)\n",
    "    print()\n",
    "    \n",
    "    # Obtain Ratio Values\n",
    "    time_days_dist_ratio = map(lambda x: (x[0], x[1]/float(len(input_data))), time_days_dist) \n",
    "    pprint(time_days_dist_ratio)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entire Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'Thu', 2689),\n",
      " (u'Fri', 2678),\n",
      " (u'Sat', 2527),\n",
      " (u'Wed', 2452),\n",
      " (u'Tue', 2435),\n",
      " (u'Sun', 2428),\n",
      " (u'Mon', 2372)]\n",
      "\n",
      "[(u'Thu', 0.15294920652977648),\n",
      " (u'Fri', 0.1523235310846937),\n",
      " (u'Sat', 0.1437347136112849),\n",
      " (u'Wed', 0.1394687446675388),\n",
      " (u'Tue', 0.13850179170695637),\n",
      " (u'Sun', 0.13810363460554007),\n",
      " (u'Mon', 0.13491837779420965)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timestamp(data_train_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clickbait Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'Thu', 685),\n",
      " (u'Fri', 676),\n",
      " (u'Sat', 661),\n",
      " (u'Wed', 623),\n",
      " (u'Tue', 617),\n",
      " (u'Mon', 591),\n",
      " (u'Sun', 580)]\n",
      "\n",
      "[(u'Thu', 0.15452289645838033),\n",
      " (u'Fri', 0.15249266862170088),\n",
      " (u'Sat', 0.14910895556056847),\n",
      " (u'Wed', 0.14053688247236634),\n",
      " (u'Tue', 0.13918339724791337),\n",
      " (u'Mon', 0.1333182946086172),\n",
      " (u'Sun', 0.13083690503045342)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timestamp(data_train_X_bait)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Clickbait Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'Thu', 2004),\n",
      " (u'Fri', 2002),\n",
      " (u'Sat', 1866),\n",
      " (u'Sun', 1848),\n",
      " (u'Wed', 1829),\n",
      " (u'Tue', 1818),\n",
      " (u'Mon', 1781)]\n",
      "\n",
      "[(u'Thu', 0.1524186188013386),\n",
      " (u'Fri', 0.1522665044113173),\n",
      " (u'Sat', 0.14192272588986918),\n",
      " (u'Sun', 0.14055369637967752),\n",
      " (u'Wed', 0.1391086096744752),\n",
      " (u'Tue', 0.13827198052935807),\n",
      " (u'Mon', 0.1354578643139641)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timestamp(data_train_X_notbait)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
